{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the Auto-encoder\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Custom loss function including both reconstruction and clustering terms\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_param, cluster_centers):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.cluster_centers = cluster_centers\n",
    "\n",
    "    def forward(self, decoded, original, encoded):\n",
    "        reconstruction_loss = nn.MSELoss()(decoded, original)\n",
    "        clustering_loss = torch.mean(torch.min(torch.sum((encoded.unsqueeze(1) - self.cluster_centers)**2, dim=2), dim=1)[0])\n",
    "        total_loss = reconstruction_loss + self.lambda_param * clustering_loss\n",
    "        return total_loss\n",
    "\n",
    "# Load and preprocess dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Training the Auto-encoder\n",
    "autoencoder = AutoEncoder(32*32*3, 256, 64).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Pretrain autoencoder\n",
    "for epoch in range(50):\n",
    "    for img, _ in data_loader:\n",
    "        img = img.view(img.size(0), -1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        decoded, encoded = autoencoder(img)\n",
    "        loss = criterion(decoded, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Extract features for clustering\n",
    "features = []\n",
    "for img, _ in data_loader:\n",
    "    img = img.view(img.size(0), -1).cuda()\n",
    "    _, encoded = autoencoder(img)\n",
    "    features.append(encoded.detach().cpu().numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "pca = PCA(n_components=50)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Perform K-Means clustering\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "clusters = kmeans.fit_predict(features_pca)\n",
    "cluster_centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float).cuda()\n",
    "\n",
    "# Fine-tune autoencoder with clustering loss\n",
    "lambda_param = 0.1\n",
    "custom_loss = CustomLoss(lambda_param, cluster_centers)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for img, _ in data_loader:\n",
    "        img = img.view(img.size(0), -1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        decoded, encoded = autoencoder(img)\n",
    "        loss = custom_loss(decoded, img, encoded)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Extract features again\n",
    "features = []\n",
    "for img, _ in data_loader:\n",
    "    img = img.view(img.size(0), -1).cuda()\n",
    "    _, encoded = autoencoder(img)\n",
    "    features.append(encoded.detach().cpu().numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "distances = np.linalg.norm(features - cluster_centers[clusters], axis=1)\n",
    "\n",
    "# Identify top anomalies\n",
    "anomaly_indices = np.argsort(distances)[-100:]\n",
    "anomalous_images = [dataset[i][0].numpy().transpose(1, 2, 0) for i in anomaly_indices]\n",
    "\n",
    "# Visualize anomalies\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((anomalous_images[i] * 255).astype(np.uint8))\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-layer feature extractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a feature extractor that combines features from multiple layers\n",
    "class MultiLayerFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MultiLayerFeatureExtractor, self).__init__()\n",
    "        self.layer1 = nn.Sequential(*list(pretrained_model.children())[:4])\n",
    "        self.layer2 = nn.Sequential(*list(pretrained_model.children())[4:5])\n",
    "        self.layer3 = nn.Sequential(*list(pretrained_model.children())[5:6])\n",
    "        self.layer4 = nn.Sequential(*list(pretrained_model.children())[6:7])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.layer1(x)\n",
    "        features.append(self.avgpool(x).view(x.size(0), -1))\n",
    "        x = self.layer2(x)\n",
    "        features.append(self.avgpool(x).view(x.size(0), -1))\n",
    "        x = self.layer3(x)\n",
    "        features.append(self.avgpool(x).view(x.size(0), -1))\n",
    "        x = self.layer4(x)\n",
    "        features.append(self.avgpool(x).view(x.size(0), -1))\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=256, shuffle=True)\n",
    "\n",
    "# Initialize the pretrained model and feature extractor\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "feature_extractor = MultiLayerFeatureExtractor(pretrained_model).cuda()\n",
    "\n",
    "# Extract and combine features\n",
    "features = []\n",
    "for img, _ in train_loader:\n",
    "    img = img.cuda()\n",
    "    with torch.no_grad():\n",
    "        combined_features = feature_extractor(img)\n",
    "    features.append(combined_features.cpu().numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "\n",
    "# Perform anomaly detection using combined features (e.g., K-Means clustering, DBSCAN, etc.)\n",
    "# For simplicity, let's apply PCA and visualize the combined features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "plt.scatter(features_pca[:, 0], features_pca[:, 1], alpha=0.5)\n",
    "plt.title(\"PCA of Combined Features\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
