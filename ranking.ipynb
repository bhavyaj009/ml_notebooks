{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a feature extractor using a pretrained model\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(pretrained_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# Define the Auto-encoder with a feature extractor\n",
    "class PretrainedAutoEncoder(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PretrainedAutoEncoder, self).__init__()\n",
    "        self.encoder = feature_extractor\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32*32*3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.view(x.size(0), 3, 32, 32), encoded\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4)\n",
    "])\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=256, shuffle=True)\n",
    "\n",
    "# Initialize models and optimizer\n",
    "pretrained_model = resnet18(pretrained=True)\n",
    "feature_extractor = FeatureExtractor(pretrained_model).cuda()\n",
    "autoencoder = PretrainedAutoEncoder(feature_extractor).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    for img, _ in train_loader:\n",
    "        img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        decoded, encoded = autoencoder(img)\n",
    "        loss = criterion(decoded, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Extract features and reconstruction errors\n",
    "features = []\n",
    "reconstruction_errors = []\n",
    "for img, _ in train_loader:\n",
    "    img = img.cuda()\n",
    "    with torch.no_grad():\n",
    "        decoded, encoded = autoencoder(img)\n",
    "        loss = torch.mean((decoded - img) ** 2, dim=[1, 2, 3])\n",
    "        reconstruction_errors.extend(loss.cpu().numpy())\n",
    "        features.extend(encoded.cpu().numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "\n",
    "# Mahalanobis Distance\n",
    "def calculate_mahalanobis_distances(features):\n",
    "    mean_vec = np.mean(features, axis=0)\n",
    "    cov_matrix = np.cov(features, rowvar=False)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    distances = [mahalanobis(f, mean_vec, inv_cov_matrix) for f in features]\n",
    "    return np.array(distances)\n",
    "\n",
    "mahalanobis_distances = calculate_mahalanobis_distances(features)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "iso_labels = iso_forest.fit_predict(features)\n",
    "anomaly_indices_iso_forest = np.where(iso_labels == -1)[0]\n",
    "\n",
    "# Combine and rank anomalies\n",
    "anomaly_scores = reconstruction_errors + mahalanobis_distances\n",
    "top_anomaly_indices = np.argsort(anomaly_scores)[-100:]\n",
    "\n",
    "anomalous_images = [cifar10_train[i][0].numpy().transpose(1, 2, 0) for i in top_anomaly_indices]\n",
    "\n",
    "# Visualize anomalies\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((anomalous_images[i] * 255).astype(np.uint8))\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
